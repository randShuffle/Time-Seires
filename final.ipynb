{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_nums=1\n",
    "\n",
    "\n",
    "# 修改后的createXY函数\n",
    "def createXY(dataset: pd.DataFrame, n_past: int, n_future: int, column_target: str):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(n_past, len(dataset) - n_future + 1):\n",
    "        dataX.append(dataset.iloc[i - n_past:i].values)\n",
    "        dataY.append(dataset.iloc[i:i + n_future][column_target].values)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# 修改后的process_files函数\n",
    "def process_files(columns_all, column_target, folder_path, n_past=1, n_future=1):\n",
    "    all_dataX, all_dataY = np.array([]), np.array([])\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            series = pd.read_csv(file_path)\n",
    "            single_dataset = series[columns_all]\n",
    "            dataX, dataY = createXY(single_dataset, n_past, n_future, column_target[0])\n",
    "            all_dataX = np.vstack([all_dataX, dataX]) if all_dataX.size else dataX\n",
    "            all_dataY = np.vstack([all_dataY, dataY]) if all_dataY.size else dataY\n",
    "    return all_dataX, all_dataY\n",
    "\n",
    "columns_all = ['CGM (mg / dl)', 'CSII - basal insulin (Novolin R, IU / H)']\n",
    "column_target = ['CGM (mg / dl)']\n",
    "folder_path = './diabetes_datasets/T1'\n",
    "\n",
    "# 使用n_past=8, n_future=4调用process_files\n",
    "dataX, dataY = process_files(columns_all, column_target, folder_path, n_past=8, n_future=output_nums)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn, optim\n",
    "\n",
    "# 假设dataX和dataY是你的数据\n",
    "# 将它们转换为PyTorch张量，这里假设它们已经是Tensor或者从Numpy转换过来的\n",
    "dataX_tensor = torch.tensor(dataX, dtype=torch.float32)\n",
    "dataY_tensor = torch.tensor(dataY, dtype=torch.float32)\n",
    "\n",
    "# 创建TensorDataset对象\n",
    "dataset = TensorDataset(dataX_tensor, dataY_tensor)\n",
    "\n",
    "# 其余拆分数据集、创建DataLoader对象的代码与之前相同\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size)\n",
    "\n",
    "\n",
    "# 随机拆分数据集\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# 创建DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "feature_nums = dataX.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改后的LSTMRegressor类\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, num_units=50, dropout=0.2, output_size=output_nums):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_nums, hidden_size=num_units, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=num_units, hidden_size=num_units, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(in_features=num_units, out_features=output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X, _ = self.lstm1(X)\n",
    "        X, _ = self.lstm2(X)\n",
    "        X = self.dropout(X)\n",
    "        X = X[:, -1, :]  # Get the last sequence output\n",
    "        X = self.dense(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 30601.103515625, Validation Loss: 28515.62468112245\n",
      "Epoch 1: Training Loss: 25802.462890625, Validation Loss: 26187.59797512755\n",
      "Epoch 2: Training Loss: 26021.341796875, Validation Loss: 24121.552853954083\n",
      "Epoch 3: Training Loss: 20677.99609375, Validation Loss: 22235.36224489796\n",
      "Epoch 4: Training Loss: 18962.0234375, Validation Loss: 20508.449398118624\n",
      "Epoch 5: Training Loss: 14620.5400390625, Validation Loss: 18912.69232302296\n",
      "Epoch 6: Training Loss: 17788.330078125, Validation Loss: 17452.179587850766\n",
      "Epoch 7: Training Loss: 19132.6015625, Validation Loss: 16112.062679368622\n",
      "Epoch 8: Training Loss: 12856.798828125, Validation Loss: 14877.438277264031\n",
      "Epoch 9: Training Loss: 12905.1806640625, Validation Loss: 13750.096859056122\n",
      "Epoch 10: Training Loss: 15616.0087890625, Validation Loss: 12723.531269929847\n",
      "Epoch 11: Training Loss: 11456.5771484375, Validation Loss: 11788.053691007653\n",
      "Epoch 12: Training Loss: 11498.2080078125, Validation Loss: 10938.547403140943\n",
      "Epoch 13: Training Loss: 11053.79296875, Validation Loss: 10174.868921396685\n",
      "Epoch 14: Training Loss: 9957.630859375, Validation Loss: 9485.053172831633\n",
      "Epoch 15: Training Loss: 6685.14013671875, Validation Loss: 8871.367207429847\n",
      "Epoch 16: Training Loss: 12756.744140625, Validation Loss: 8325.459652024872\n",
      "Epoch 17: Training Loss: 5969.48486328125, Validation Loss: 7841.174874441965\n",
      "Epoch 18: Training Loss: 9558.431640625, Validation Loss: 7417.133335658482\n",
      "Epoch 19: Training Loss: 7265.05224609375, Validation Loss: 6425.291015625\n",
      "Epoch 20: Training Loss: 9546.5810546875, Validation Loss: 5892.839599609375\n",
      "Epoch 21: Training Loss: 4525.708984375, Validation Loss: 5408.3530472735965\n",
      "Epoch 22: Training Loss: 7963.046875, Validation Loss: 4970.412627551021\n",
      "Epoch 23: Training Loss: 2632.738525390625, Validation Loss: 4563.654371611926\n",
      "Epoch 24: Training Loss: 3049.701416015625, Validation Loss: 4202.716781927615\n",
      "Epoch 25: Training Loss: 4114.39404296875, Validation Loss: 3856.9396150550065\n",
      "Epoch 26: Training Loss: 3106.8388671875, Validation Loss: 3552.025933713329\n",
      "Epoch 27: Training Loss: 3842.47021484375, Validation Loss: 3265.691336495536\n",
      "Epoch 28: Training Loss: 3054.844482421875, Validation Loss: 2999.848247917331\n",
      "Epoch 29: Training Loss: 3288.2890625, Validation Loss: 2772.4256130919166\n",
      "Epoch 30: Training Loss: 3047.44140625, Validation Loss: 2536.5063887615593\n",
      "Epoch 31: Training Loss: 1390.4793701171875, Validation Loss: 2333.980060188138\n",
      "Epoch 32: Training Loss: 2095.2275390625, Validation Loss: 2149.7640119280136\n",
      "Epoch 33: Training Loss: 2350.04345703125, Validation Loss: 1981.3182547433037\n",
      "Epoch 34: Training Loss: 2528.58740234375, Validation Loss: 1820.909872249681\n",
      "Epoch 35: Training Loss: 2802.419677734375, Validation Loss: 1670.9068809042171\n",
      "Epoch 36: Training Loss: 2031.565185546875, Validation Loss: 1537.6404163594148\n",
      "Epoch 37: Training Loss: 894.5950317382812, Validation Loss: 1415.493168422154\n",
      "Epoch 38: Training Loss: 1229.41259765625, Validation Loss: 1303.713875906808\n",
      "Epoch 39: Training Loss: 1264.7899169921875, Validation Loss: 1197.8014277244101\n",
      "Epoch 40: Training Loss: 2664.219482421875, Validation Loss: 1109.4259170220823\n",
      "Epoch 41: Training Loss: 2133.61279296875, Validation Loss: 1016.3186928885324\n",
      "Epoch 42: Training Loss: 977.2600708007812, Validation Loss: 935.7806271922832\n",
      "Epoch 43: Training Loss: 1572.5093994140625, Validation Loss: 869.9752848099689\n",
      "Epoch 44: Training Loss: 1188.845947265625, Validation Loss: 795.3890415113799\n",
      "Epoch 45: Training Loss: 458.2720642089844, Validation Loss: 754.9402615294165\n",
      "Epoch 46: Training Loss: 695.583984375, Validation Loss: 678.0707191934391\n",
      "Epoch 47: Training Loss: 1085.7762451171875, Validation Loss: 628.8395211356027\n",
      "Epoch 48: Training Loss: 259.24639892578125, Validation Loss: 600.9139223682637\n",
      "Epoch 49: Training Loss: 1583.56591796875, Validation Loss: 539.3353218545719\n",
      "Epoch 50: Training Loss: 422.80731201171875, Validation Loss: 499.7625807158801\n",
      "Epoch 51: Training Loss: 263.07171630859375, Validation Loss: 473.89869876783723\n",
      "Epoch 52: Training Loss: 400.0186767578125, Validation Loss: 431.94048698580997\n",
      "Epoch 53: Training Loss: 498.17108154296875, Validation Loss: 410.78397260393416\n",
      "Epoch 54: Training Loss: 1196.0594482421875, Validation Loss: 374.56908089774\n",
      "Epoch 55: Training Loss: 412.8607482910156, Validation Loss: 349.367871498575\n",
      "Epoch 56: Training Loss: 390.1517333984375, Validation Loss: 343.08765364666374\n",
      "Epoch 57: Training Loss: 355.3962097167969, Validation Loss: 305.81139778604313\n",
      "Epoch 58: Training Loss: 249.45115661621094, Validation Loss: 289.39078552868904\n",
      "Epoch 59: Training Loss: 414.73626708984375, Validation Loss: 270.8390000012456\n",
      "Epoch 60: Training Loss: 541.8643798828125, Validation Loss: 255.65423786396883\n",
      "Epoch 61: Training Loss: 257.6881408691406, Validation Loss: 244.20031940693758\n",
      "Epoch 62: Training Loss: 316.8733825683594, Validation Loss: 225.566927462208\n",
      "Epoch 63: Training Loss: 259.73016357421875, Validation Loss: 211.0011658960459\n",
      "Epoch 64: Training Loss: 329.20025634765625, Validation Loss: 203.0793179881816\n",
      "Epoch 65: Training Loss: 178.40509033203125, Validation Loss: 188.8404210927535\n",
      "Epoch 66: Training Loss: 201.64065551757812, Validation Loss: 179.08750596338388\n",
      "Epoch 67: Training Loss: 775.462158203125, Validation Loss: 174.06094757391483\n",
      "Epoch 68: Training Loss: 405.6839294433594, Validation Loss: 166.57478893046476\n",
      "Epoch 69: Training Loss: 520.1105346679688, Validation Loss: 175.17045110585738\n",
      "Epoch 70: Training Loss: 365.63330078125, Validation Loss: 151.68954055163326\n",
      "Epoch 71: Training Loss: 229.43243408203125, Validation Loss: 145.24481987466618\n",
      "Epoch 72: Training Loss: 191.2310028076172, Validation Loss: 135.66144374925264\n",
      "Epoch 73: Training Loss: 127.4717025756836, Validation Loss: 144.0421496800014\n",
      "Epoch 74: Training Loss: 317.1889343261719, Validation Loss: 128.46740960101693\n",
      "Epoch 75: Training Loss: 245.91346740722656, Validation Loss: 121.31725887376435\n",
      "Epoch 76: Training Loss: 187.74374389648438, Validation Loss: 123.5306805591194\n",
      "Epoch 77: Training Loss: 214.4491729736328, Validation Loss: 121.18999940521863\n",
      "Epoch 78: Training Loss: 219.05751037597656, Validation Loss: 114.9386479124731\n",
      "Epoch 79: Training Loss: 196.49913024902344, Validation Loss: 101.24153211165448\n",
      "Epoch 80: Training Loss: 266.2447814941406, Validation Loss: 101.3921674611617\n",
      "Epoch 81: Training Loss: 239.86314392089844, Validation Loss: 113.31194546757912\n",
      "Epoch 82: Training Loss: 525.4411010742188, Validation Loss: 131.17913078775211\n",
      "Epoch 83: Training Loss: 149.11541748046875, Validation Loss: 89.1785769949154\n",
      "Epoch 84: Training Loss: 167.41172790527344, Validation Loss: 87.63059437031649\n",
      "Epoch 85: Training Loss: 539.6472778320312, Validation Loss: 86.05549905738052\n",
      "Epoch 86: Training Loss: 177.95712280273438, Validation Loss: 86.17832946777344\n",
      "Epoch 87: Training Loss: 189.81997680664062, Validation Loss: 84.74573649192342\n",
      "Epoch 88: Training Loss: 154.1226348876953, Validation Loss: 83.50411208795042\n",
      "Epoch 89: Training Loss: 312.3313293457031, Validation Loss: 89.0848331451416\n",
      "Epoch 90: Training Loss: 274.95611572265625, Validation Loss: 79.15452423874213\n",
      "Epoch 91: Training Loss: 261.80548095703125, Validation Loss: 74.87752965031838\n",
      "Epoch 92: Training Loss: 276.68707275390625, Validation Loss: 71.00445930325255\n",
      "Epoch 93: Training Loss: 323.7843322753906, Validation Loss: 78.85089928763253\n",
      "Epoch 94: Training Loss: 219.81777954101562, Validation Loss: 71.20762260592714\n",
      "Epoch 95: Training Loss: 132.30894470214844, Validation Loss: 69.6024941814189\n",
      "Epoch 96: Training Loss: 358.2167053222656, Validation Loss: 66.1701209399165\n",
      "Epoch 97: Training Loss: 262.0312194824219, Validation Loss: 77.05954902026119\n",
      "Epoch 98: Training Loss: 192.30697631835938, Validation Loss: 71.96418088796187\n",
      "Epoch 99: Training Loss: 284.2496643066406, Validation Loss: 64.76764351981026\n"
     ]
    }
   ],
   "source": [
    "model = LSTMRegressor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 早停法参数\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "max_epochs = 100\n",
    "\n",
    "# 训练过程\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Training loop\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 每个epoch后，在验证集上评估模型\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "        \n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch}: Training Loss: {loss.item()}, Validation Loss: {val_loss}')\n",
    "\n",
    "    # 早停法逻辑\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 保存最好的模型状态\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping triggered after {epoch} epochs.')\n",
    "        model.load_state_dict(best_model_state)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 57.204752201936685\n"
     ]
    }
   ],
   "source": [
    "# 测试过程\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
