{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_nums=4\n",
    "\n",
    "\n",
    "# 修改后的createXY函数\n",
    "def createXY(dataset: pd.DataFrame, n_past: int, n_future: int, column_target: str):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(n_past, len(dataset) - n_future + 1):\n",
    "        dataX.append(dataset.iloc[i - n_past:i].values)\n",
    "        dataY.append(dataset.iloc[i:i + n_future][column_target].values)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# 修改后的process_files函数\n",
    "def process_files(columns_all, column_target, folder_path, n_past=1, n_future=1):\n",
    "    all_dataX, all_dataY = np.array([]), np.array([])\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            series = pd.read_csv(file_path)\n",
    "            single_dataset = series[columns_all]\n",
    "            dataX, dataY = createXY(single_dataset, n_past, n_future, column_target)\n",
    "            all_dataX = np.vstack([all_dataX, dataX]) if all_dataX.size else dataX\n",
    "            all_dataY = np.vstack([all_dataY, dataY]) if all_dataY.size else dataY\n",
    "    return all_dataX, all_dataY\n",
    "\n",
    "\n",
    "columns_all=['CGM (mg / dl)','insulin','carbohydrate','protein','fat','cellulose','hour_sin','hour_cos','CGM_diff']\n",
    "# columns_all = ['CGM (mg / dl)','CSII - basal insulin (Novolin R, IU / H)']\n",
    "column_target = ['CGM (mg / dl)']\n",
    "folder_path = '../diabetes_datasets/T2'\n",
    "\n",
    "# 使用n_past=8, n_future=4调用process_files\n",
    "n_past=8\n",
    "dataX, dataY = process_files(columns_all, column_target, folder_path, n_past=n_past, n_future=output_nums)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataY_for_train=dataY[:,0,:].reshape(-1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111142, 8, 9), (111142, 4, 1), (111142, 1, 1))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX.shape,dataY.shape,dataY_for_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[68.4],\n",
       "        [75.6],\n",
       "        [61.2],\n",
       "        [48.6]]),\n",
       " array([[68.4]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataY[0],dataY_for_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# 假设dataX和dataY是你的数据\n",
    "# 将它们转换为PyTorch张量，这里假设它们已经是Tensor或者从Numpy转换过来的\n",
    "dataX_tensor = torch.tensor(dataX, dtype=torch.float32)\n",
    "dataY_tensor = torch.tensor(dataY_for_train, dtype=torch.float32)\n",
    "\n",
    "# 创建TensorDataset对象\n",
    "dataset = TensorDataset(dataX_tensor, dataY_tensor)\n",
    "\n",
    "# 其余拆分数据集、创建DataLoader对象的代码与之前相同\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size)\n",
    "\n",
    "\n",
    "# # 随机拆分数据集\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "assert train_size + val_size + test_size == len(dataset)\n",
    "# 按照顺序划分数据集\n",
    "train_dataset = Subset(dataset, range(0, train_size))\n",
    "val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "test_dataset = Subset(dataset, range(train_size + val_size, len(dataset)))\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "# 创建DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "feature_nums = dataX.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, input_dim=feature_nums, output_dim=1, d_model=4, nhead=2, num_encoder_layers=1, dropout=0.2):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        \n",
    "        # 使用线性层代替嵌入层\n",
    "        self.linear_input = nn.Linear(self.input_dim, self.d_model)\n",
    "\n",
    "        # 多头自注意力层\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=self.num_encoder_layers)\n",
    "\n",
    "        # 输出层\n",
    "        self.linear_out = nn.Linear(self.d_model, self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过线性层\n",
    "        x = self.linear_input(x)\n",
    "\n",
    "        # 通过Transformer编码器\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 取最后一个时间步的输出\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # 通过输出层\n",
    "        x = self.linear_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从零训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerEncoderModel().to(device)\n",
    "# criterion = nn.L1Loss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # 早停法参数\n",
    "# best_val_loss = float('inf')\n",
    "# patience = 10\n",
    "# patience_counter = 0\n",
    "# max_epochs = 5000\n",
    "\n",
    "# for epoch in range(max_epochs):\n",
    "#     train_loss = 0\n",
    "#     model.train()\n",
    "    \n",
    "#     # Training loop\n",
    "#     for data, target in train_loader:\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         target = target.squeeze(-1)  # 调整目标尺寸\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = criterion(output, target)\n",
    "#         train_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     # 每个epoch后，在验证集上评估模型\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         for data, target in val_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             target = target.squeeze(-1)  # 调整目标尺寸\n",
    "#             output = model(data)\n",
    "#             val_loss += criterion(output, target).item()\n",
    "    \n",
    "#     train_loss /= len(train_loader)\n",
    "#     val_loss /= len(val_loader)\n",
    "\n",
    "#     print(f'Epoch {epoch}: Training Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#     # 早停法逻辑\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         patience_counter = 0\n",
    "#         # 保存最好的模型状态\n",
    "#         best_model_state = model.state_dict()\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "\n",
    "#     if patience_counter >= patience:\n",
    "#         print(f'Early stopping triggered after {epoch} epochs.')\n",
    "#         model.load_state_dict(best_model_state)\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 保存模型的状态字典到文件\n",
    "# torch.save(model.state_dict(), '../model/T2/transformerEncoderMultiStep.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataY_tensor = torch.tensor(dataY, dtype=torch.float32)\n",
    "# 创建TensorDataset对象\n",
    "dataset = TensorDataset(dataX_tensor, dataY_tensor)\n",
    "# 其余拆分数据集、创建DataLoader对象的代码与之前相同\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size)\n",
    "assert train_size + val_size + test_size == len(dataset)\n",
    "test_dataset = Subset(dataset, range(train_size + val_size, len(dataset)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 加载最佳模型状态\n",
    "# model.load_state_dict(best_model_state)\n",
    "# criterion_mae=nn.L1Loss()\n",
    "# # 在测试集上验证模型并输出预测标签\n",
    "# model.eval()\n",
    "# test_loss = 0\n",
    "# test_loss_mae=0\n",
    "# all_predictions = []\n",
    "# all_targets = []\n",
    "# with torch.no_grad():\n",
    "#     for data, target in test_loader:\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "        \n",
    "#         target = target.squeeze(-1)  # 调整目标尺寸\n",
    "#         output = model(data)\n",
    "#         test_loss += criterion(output, target).item()\n",
    "#         test_loss_mae+=criterion_mae(output, target).item()\n",
    "#         all_predictions.append(output.cpu().numpy())\n",
    "#         all_targets.append(target.cpu().numpy())\n",
    "\n",
    "# test_loss /= len(test_loader)\n",
    "# test_loss_mae/=len(test_loader)\n",
    "# print(f'Test Loss: {test_loss}')\n",
    "# print(f'Test Loss mae: {test_loss_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从磁盘上load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=TransformerEncoderModel().to(device)\n",
    "model.load_state_dict(torch.load('../model/T2/TransformerEncoder.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 9 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\DataMining\\Time-Seires\\T2\\transformer_encoder_multi_step.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataMining/Time-Seires/T2/transformer_encoder_multi_step.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(output_nums):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataMining/Time-Seires/T2/transformer_encoder_multi_step.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DataMining/Time-Seires/T2/transformer_encoder_multi_step.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     data\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mcat((data[:,\u001b[39m1\u001b[39;49m:,:],target),\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataMining/Time-Seires/T2/transformer_encoder_multi_step.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DataMining/Time-Seires/T2/transformer_encoder_multi_step.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     output \u001b[39m=\u001b[39m model(data)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 9 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "criterion=nn.L1Loss()\n",
    "# 在测试集上验证模型并输出预测标签\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # data:[16, 8, 9] target:[16, 4, 1],output:[16,1]\n",
    "        for i in range(output_nums):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            target = target.squeeze(-1)\n",
    "            output = model(data)\n",
    "            # 将output变为[16,1,1]的形式\n",
    "            output=output.unsqueeze(1)\n",
    "            # output第一维加上其他特征，其他特征分别是下一个data的第2维到第9维\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # 将data的第1个维度集体向左平移，再加上output，例如[1,2,3,4,5,6,7,8]->[2,3,4,5,6,7,8,prediction]\n",
    "            data=torch.cat((data[:,1:,:],output),1)\n",
    "            print(data)\n",
    "            print(output)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            all_predictions.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "            break\n",
    "\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化分别看1步-4步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
